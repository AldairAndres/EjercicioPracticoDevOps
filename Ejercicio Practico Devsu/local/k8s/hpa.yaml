apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: prueba-java-hpa          # Nombre identificador del HPA
  # labels o annotations adicionales podr√≠an incluir informaci√≥n de owner,
  # o marker para integraci√≥n con herramientas de GitOps, etc.
  namespace: namespace-prueba-java
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: prueba-java-app        # Debe coincidir con el nombre del Deployment
  minReplicas: 2                 # üîπ M√≠nimo de r√©plicas: garantiza capacidad base
  maxReplicas: 5                 # üîπ M√°ximo de r√©plicas: evita escalar sin fin
  metrics:
    - type: Resource
      resource:
        name: cpu                # üîπ M√©trica basada en CPU; alternativa: memory, custom
        target:
          type: Utilization
          averageUtilization: 50 # üîπ Objetivo: el 50% de uso de CPU en promedio

  # ---------------------------------------------
  # Buenas pr√°cticas al configurar un HPA:
  # 1) Definir requests en el Deployment:
  #    - Asegura de que cada contenedor tenga 'resources.requests.cpu' configurado,
  #      porque el HPA calcula el porcentaje de uso respecto a ese request.
  #
  # 2) Escoger m√©tricas adecuadas:
  #    - CPU es la m√°s com√∫n, pero si la app es I/O-bound, se puede considerar memory o custom metrics.
  #
  # 3) Valores de min/max realistas:
  #    - minReplicas: suficiente para manejar carga m√≠nima y preservar estado.
  #    - maxReplicas: suficiente para picos, pero sin sobrecargar el cluster.
  #
  # 4) Ajustar thresholds de Utilization:
  #    - Un target de 50‚Äì60% es est√°ndar; m√°s bajo => m√°s r√©plicas (mejor latencia, m√°s coste);
  #      m√°s alto => menos r√©plicas (ahorro, posible latencia).
  #
  # 5) Probes configuradas en el Deployment:
  #    - tenemos liveness/readiness para que el HPA no cuente pods en estado NotReady.
  #
  # 6) Estabilidad del escalado:
  #    - Kubernetes v2 HPA soporta behavior policies para controlar la velocidad de scalingUp/Down.
  #      Podemos a√±adir:
  #        behavior:
  #          scaleUp:
  #            policies:
  #              - type: Percent
  #                value: 50
  #                periodSeconds: 60
  #          scaleDown:
  #            policies:
  #              - type: Pods
  #                value: 1
  #                periodSeconds: 30
  #    - Esto evita rampas de r√©plicas muy agresivas.
  #
  # 7) Monitorizaci√≥n y alertas:
  #    - Para un monitoreo mas avanzado, se puede implementar dashboards o alerts (en Prometheus/Grafana, Azure Monitor‚Ä¶) para cuando el HPA llegue al m√°ximo o m√≠nimo.
  #
  # 8) Pruebas de carga:
  #    - Se pueden simular tr√°fico con herramientas (ab, JMeter, k6) y observar el comportamiento del HPA antes de producci√≥n.
  #
